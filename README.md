# FSDPToys - Lightweight FSDP Training Framework

FSDPToys is a lightweight repository for large model training based on PyTorch FSDP (Fully Sharded Data Parallel). It provides simple and universal distributed training code without requiring pre-trained weights, and ensures perfect loss alignment between FSDP1 and FSDP2 implementations.

## ðŸš€ Features

- **Efficient Distributed Training**: Native PyTorch FSDP/FSDP2 implementation with sharded parameters, gradients, and optimizer states
- **Hybrid Sharding (HSDP)**: Flexible sharding strategies via Device Mesh configuration
- **Memory Optimization**:
  - Gradient Checkpointing
  - CPU Offloading (parameters, gradients, optimizer states)
  - Chunk Loss for reduced peak memory usage
  - Optimizer offload (only for optimizer states)
  - FSDP stream reuse patches for FSDP1/FSDP2
- **Hardware Compatibility**: Supports both NVIDIA GPU (CUDA) and Huawei Ascend NPU
- **Built-in Profiler and Snapshot**: Performance analysis tools for GPU/NPU profiling

## ðŸ“ Directory Structure

```
FSDPToys/
â”œâ”€â”€ accelerate/              # Acceleration components
â”‚   â”œâ”€â”€ fsdp1_patch.py       
â”‚   â”œâ”€â”€ fsdp2_patch.py      
â”‚   â”œâ”€â”€ loss.py              # Chunk Loss
â”‚   â”œâ”€â”€ offload.py           # Opitimizer Offloading
â”‚   â””â”€â”€ recompute.py         
â”œâ”€â”€ models/                  # Model implementations
â”‚   â”œâ”€â”€ llama2.py            # Llama2 model
â”‚   â”œâ”€â”€ llama3.py            
â”‚   â”œâ”€â”€ llama4/              
â”‚   â””â”€â”€ qwen3_moe_mini.py    
â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”œâ”€â”€ profiler.py          # Performance profiler
â”‚   â””â”€â”€ utils.py             
â”œâ”€â”€ train_fsdp1.py           # FSDP1 training script
â”œâ”€â”€ train_fsdp2.py           # FSDP2 training script
â”œâ”€â”€ train_simple.py          # Simple single-GPU training script
â”œâ”€â”€ run_fsdp.sh
â”œâ”€â”€ test.py
â””â”€â”€ README.md                
```

## âš™ï¸ Requirements

- **Python**: >= 3.10
- **PyTorch**: >= 2.6
- Torch_npu: https://gitcode.com/Ascend/pytorch

## ðŸš¦Quick Start

### Using Launch Script
The pre-configured script automatically detects hardware and starts training:

```bash
bash run_fsdp.sh
```

### Manual Training
Customize training with `torchrun`:

```bash
# FSDP1 example
torchrun --nnodes=1 --nproc_per_node=4 train_fsdp1.py \
  --batch_size=4 \
  --seq_len=4096 \
  --fsdp_size=8 \
  --gradient_checkpointing \
  --chunk_loss \
  --cpu_offload

# FSDP2 example
torchrun --nnodes=1 --nproc_per_node=8 train_fsdp2.py \
  --batch_size=4\
  --seq_len=4096 \
  --fsdp_size=8 \
  --gradient_checkpointing
```

## ðŸ”§ Key Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--batch_size` | int | 4 | Batch size per GPU |
| `--seq_len` | int | 1024 | Input sequence length |
| `--fsdp_size` | int | 8 | HSDP sharding size |
| `--cpu_offload` | bool | False | Enable CPU offloading |
| `--optimizer_offload` | bool | False | Offload optimizer states only |
| `--gradient_checkpointing` | bool | False | Enable gradient checkpointing |
| `--chunk_loss` | bool | False | Enable chunked loss computation |
| `--profile` | bool | False | Enable PyTorch profiling |
| `--snapshot` | bool | False | Enable PyTorch memory snapshot |

## ðŸ’¡ Memory Optimization Tips

1. `fsdp_size=world_size`
2. `--gradient_checkpointing`
3. ` --chunk_loss`
4. `--cpu_offload` or `--optimizer_offload`
